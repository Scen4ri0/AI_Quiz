[
  { "id": "q1",  "question": "Из чего обычно состоит обучающая выборка больших языковых моделей?" },
  { "id": "q2",  "question": "Что такое токенизация в контексте LLM?" },
  { "id": "q3",  "question": "Как LLM прогнозирует следующий токен (в целом — короткий ответ)?" },
  { "id": "q4",  "question": "Что такое контекстное окно в LLM?" },
  { "id": "q5",  "question": "Что такое обучение с подкреплением с человеческой обратной связью (RLHF)?" },
  { "id": "q6",  "question": "Почему модели могут галлюцинировать (т.е. выдавать неправдивую информацию)?" },
  { "id": "q7",  "question": "Какие способы есть для уменьшения ошибок в ответах модели?" },
  { "id": "q8",  "question": "Что такое fine-tuning (post-training)?" },
  { "id": "q9",  "question": "Чем отличаются данные для pre-training и post-training языковой модели?" },
  { "id": "q10", "question": "Почему качество данных обучения важно не меньше, чем их объём?" },
  { "id": "q11", "question": "В чём разница между базовой моделью и инструктивно обученной (instruction-tuned)?" },
  { "id": "q12", "question": "Почему LLM может уверенно отвечать даже при отсутствии знаний по теме?" },
  { "id": "q13", "question": "Почему LLM плохо справляется с точными вычислениями без внешних инструментов?" },
  { "id": "q14", "question": "Почему LLM не обновляет свои знания автоматически?" },
  { "id": "q15", "question": "Что означает, что LLM является вероятностной моделью?" },
  { "id": "q16", "question": "Почему ответы LLM не всегда воспроизводимы дословно?" },
  { "id": "q17", "question": "Что такое zero-shot и few-shot prompting?" },
  { "id": "q18", "question": "Почему примеры в промпте могут улучшать качество ответа?" },
  { "id": "q19", "question": "Почему при генерации длинного текста качество может снижаться?" },
  { "id": "q20", "question": "Что означает, что LLM не имеет доступа к внешнему миру по умолчанию?" }
]
